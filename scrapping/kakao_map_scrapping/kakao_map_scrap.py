import pandas as pd

# Jupyternotebook(ë˜ëŠ” ipython)ì—ì„œ ê²½ê³  ë©”ì‹œì§€ë¥¼ ë¬´ì‹œí•˜ê³  ì‹¶ì„ ë•Œ:
# import warnings
# warnings.filterwarnings("ignore")
from selenium import webdriver
from bs4 import BeautifulSoup
import time
import chromedriver_autoinstaller

###################################
# ğŸ” ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”
query = "ë§ˆí¬êµ¬ì²­ì—­ ë§›ì§‘"
###################################


# chromedriverì˜ path ì„¤ì • í›„ ì‹¤í–‰
path = chromedriver_autoinstaller.install()
driver = webdriver.Chrome(path)

columns = [
    "personal_star",
    "review",
    "url",
    "title",
    "category",
    "total_star",
    "location",
]
df = pd.DataFrame(columns=columns)


# í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ ì£¼ì†Œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
source_url = "https://map.kakao.com/"


# ì¹´ì¹´ì˜¤ ì§€ë„ì— ì ‘ì†í•©ë‹ˆë‹¤
driver.get(source_url)

# ê²€ìƒ‰ì°½ì— ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤
searchbox = driver.find_element_by_xpath("//input[@id='search.keyword.query']")
searchbox.send_keys(query)

# ê²€ìƒ‰ë²„íŠ¼ì„ ëˆŒëŸ¬ì„œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
searchbutton = driver.find_element_by_xpath("//button[@id='search.keyword.submit']")
driver.execute_script("arguments[0].click();", searchbutton)

# ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ì‹œê°„ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤
time.sleep(2)

# ê²€ìƒ‰ ê²°ê³¼ì˜ í˜ì´ì§€ ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤
html = driver.page_source

# BeautifulSoupì„ ì´ìš©í•˜ì—¬ html ì •ë³´ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤
soup = BeautifulSoup(html, "html.parser")
moreviews = soup.find_all(name="a", attrs={"class": "moreview"})
titles = soup.find_all(name="a", attrs={"class": "link_name"})
categories = soup.find_all(name="span", attrs={"class": "subcategory clickable"})
total_stars = soup.find_all(name="em", attrs={"class": "num"})
# print(stars)

# aíƒœê·¸ì˜ href ì†ì„±ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•˜ì—¬, í¬ë¡¤ë§ í•  í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
page_urls = []
page_names = []
page_categories = []
page_total_stars = []

for title, moreview, category, total_star in zip(
    titles, moreviews, categories, total_stars
):
    page_name = title.get_text()
    # print(page_name)
    page_names.append(page_name)
    # print(page_names)

    page_url = moreview.get("href")
    # print(page_url)
    page_urls.append(page_url)
    # print(page_urls)

    page_category = category.get_text()
    # print(page_category)
    page_categories.append(page_category)
    # print(page_categories)

    page_total_star = total_star.get_text()
    page_total_stars.append(page_total_star)

for page_url, page_name, page_category, page_total_star in zip(
    page_urls, page_names, page_categories, page_total_stars
):  # ë‘ ê°œì˜ ë³€ìˆ˜ë¥¼ ë°›ì•„
    # print(page_name, page_url)

    # ìƒì„¸ë³´ê¸° í˜ì´ì§€ì— ì ‘ì†í•©ë‹ˆë‹¤
    driver.get(page_url)
    time.sleep(2)

    # ì²« í˜ì´ì§€ ë¦¬ë·°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")
    contents_div = soup.find(name="div", attrs={"class": "evaluation_review"})

    location = soup.find(name="span", attrs={"class": "txt_address"})  # ìœ„ì¹˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    loacation_big = location.text.split(" ")[:2][0]
    loacation_small = location.text.split(" ")[:2][1].replace("/n", "")
    full_location = loacation_big + " " + loacation_small
    print(full_location, end="")
    try:
        # ë³„ì ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        rates = contents_div.find_all(name="em", attrs={"class": "num_rate"})
        # ë¦¬ë·°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
        reviews = contents_div.find_all(name="p", attrs={"class": "txt_comment"})
    except AttributeError as e:  # ë³„ì , ë¦¬ë·° ì—†ì„ë•Œ pass
        pass

    for rate, review in zip(rates, reviews):
        row = [
            rate.text[0],
            review.find(name="span").text,
            page_url,
            page_name,
            page_category,
            page_total_star,
            full_location,
        ]  # ë„£ì„ ë°ì´í„°
        series = pd.Series(row, index=df.columns)
        df = df.append(series, ignore_index=True)

    # 2-3í˜ì´ì§€ì˜ ë¦¬ë·°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤
    for button_num in range(2, 3):
        # ì˜¤ë¥˜ê°€ ë‚˜ëŠ” ê²½ìš°(ë¦¬ë·° í˜ì´ì§€ê°€ ì—†ëŠ” ê²½ìš°), ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        try:
            another_reviews = driver.find_element_by_xpath(
                "//a[@data-page='" + str(button_num) + "']"
            )
            another_reviews.click()
            time.sleep(2)

            # í˜ì´ì§€ ë¦¬ë·°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤
            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            contents_div = soup.find(name="div", attrs={"class": "evaluation_review"})

            # ë³„ì ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
            rates = contents_div.find_all(name="em", attrs={"class": "num_rate"})

            # ë¦¬ë·°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
            reviews = contents_div.find_all(name="p", attrs={"class": "txt_comment"})

            for rate, review in zip(rates, reviews):
                row = [rate.text[0], review.find(name="span").text]
                series = pd.Series(row, index=df.columns)
                df = df.append(series, ignore_index=True)
        except:
            break

driver.close()

# 4ì  ì´ìƒì˜ ë¦¬ë·°ëŠ” ê¸ì • ë¦¬ë·°, 3ì  ì´í•˜ì˜ ë¦¬ë·°ëŠ” ë¶€ì • ë¦¬ë·°ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
df["label"] = df["personal_star"].apply(lambda x: 1 if float(x) > 3 else 0)
# print(df.shape)
# print(df.head())

df.to_csv("kakao({}).csv".format(query), index=False)

print("csv íŒŒì¼ ìƒì„± ì™„ë£Œ!")
